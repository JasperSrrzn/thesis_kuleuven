{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext gvmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasper/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from problog.engine import DefaultEngine\n",
    "from problog.formula import LogicFormula\n",
    "from problog.logic import *\n",
    "from problog.program import PrologString\n",
    "from data_processing import clear_data_part\n",
    "from classifiers import SimpleClassifier\n",
    "import pandas as pd\n",
    "import problog\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import friedmanchisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_enta = pd.read_csv('entity_embeddingsa.csv')\n",
    "embeddings_entb = pd.read_csv('entity_embeddingsb.csv')\n",
    "embeddings_rel = pd.read_csv('relation_embeddings.csv')\n",
    "embeddings_rule = pd.read_csv('rule_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_rule = embeddings_rule.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_neighbors.json') as json_file:\n",
    "    neighbors = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening facts and rules\n"
     ]
    }
   ],
   "source": [
    "print('opening facts and rules')\n",
    "with open('rules_and_facts.txt', 'r') as myfile:\n",
    "    facts_and_rules = myfile.readlines()\n",
    "\n",
    "facts_and_rules_Prolog = PrologString(\"\\n\".join(facts_and_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_data = test_data[['entity a','entity b','rel id']]\n",
    "test_data = test_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity a    2600\n",
      "entity b    3465\n",
      "rel id        69\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "row = test_data.iloc[0]\n",
    "print(row)\n",
    "rel_id = row['rel id']\n",
    "ent_a = row['entity a']\n",
    "ent_b = row['entity b']\n",
    "query = Term('q'+str(rel_id),Term('ent'+str(ent_a)),Term('ent'+str(ent_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_action(action,predicates,embeddings_enta,embeddings_entb,embeddings_rel,embeddings_rule,randenta,randentb):\n",
    "    \"\"\"\n",
    "    function that concatenates the embeddings given the action = relation, entity a, entity b and rule\n",
    "    \"\"\"\n",
    "    node = action[1]\n",
    "    functor = db.get_node(node).functor\n",
    "    for pred in predicates:\n",
    "        if pred.functor == functor:\n",
    "            enta = term2str(pred.args[0])\n",
    "            entb = term2str(pred.args[1])\n",
    "            if enta.find('ent')==-1:\n",
    "                enta = randenta\n",
    "                enta_emb = embeddings_enta[embeddings_enta['entity a']==enta]\n",
    "                enta_emb = enta_emb.drop(['entity a'],axis=1)\n",
    "                enta_emb = enta_emb.values.tolist()[0]\n",
    "            else:\n",
    "                enta = int(enta[enta.find('ent')+3:])\n",
    "                enta_emb = embeddings_enta[embeddings_enta['entity a']==enta]\n",
    "                enta_emb = enta_emb.drop(['entity a'],axis=1)\n",
    "                enta_emb = enta_emb.values.tolist()[0]\n",
    "\n",
    "            if entb.find('ent')==-1:\n",
    "                entb = randentb\n",
    "                entb_emb = embeddings_entb[embeddings_entb['entity b']==entb]\n",
    "                entb_emb = entb_emb.drop(['entity b'],axis=1)\n",
    "                entb_emb = entb_emb.values.tolist()[0]\n",
    "            else:\n",
    "                entb = int(entb[entb.find('ent')+3:])\n",
    "                entb_emb = embeddings_entb[embeddings_entb['entity b']==entb]\n",
    "                entb_emb = entb_emb.drop(['entity b'],axis=1)\n",
    "                entb_emb = entb_emb.values.tolist()[0]\n",
    "\n",
    "\n",
    "    rule_id = db.get_node(node).args[-1]\n",
    "    rel_id = int(functor[functor.find('rel')+3:])\n",
    "    rule_emb = embeddings_rule[(embeddings_rule['rule id']==rule_id) & (embeddings_rule['rel id']==rel_id)]\n",
    "    rule_emb = rule_emb.drop(['rel id','rule id'],axis=1)\n",
    "    rule_emb = rule_emb.values.tolist()[0]\n",
    "    rel_emb = embeddings_rel[embeddings_rel['rel id']==rel_id]\n",
    "    rel_emb = rel_emb.drop(['rel id'],axis=1)\n",
    "    rel_emb = rel_emb.values.tolist()[0]\n",
    "\n",
    "\n",
    "    features = enta_emb + entb_emb + rel_emb + rule_emb\n",
    "    return [rel_id] + [enta] + [entb] + [rule_id] + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_actions(actions,predicates,embeddings_enta,embeddings_entb,embeddings_rel,embeddings_rule,model,neighbors,rand_dict):\n",
    "    actions_to_rerank = []\n",
    "    reranked_actions = []\n",
    "    pos = 0\n",
    "    for act in actions:\n",
    "        node = act[1]\n",
    "        if (db.get_node(node).functor,db.get_node(node).args[:-1]) in rand_dict.keys():\n",
    "            randenta = rand_dict[(db.get_node(node).functor,db.get_node(node).args[:-1])][0]\n",
    "            randentb = rand_dict[(db.get_node(node).functor,db.get_node(node).args[:-1])][-1]\n",
    "        else:\n",
    "            used_preds = []\n",
    "            pred_pos = 0\n",
    "            for pred in predicates:\n",
    "                if pred.functor == db.get_node(node).functor and pred_pos not in used_preds:\n",
    "                    used_preds.append(pred_pos)\n",
    "                    pred_pos+=1\n",
    "                    enta = term2str(pred.args[0])\n",
    "                    entb = term2str(pred.args[1])\n",
    "                    if enta.find('ent')==-1:\n",
    "                        entb = int(entb[entb.find('ent')+3:])\n",
    "                        randentb = entb\n",
    "                        randenta = neighbors[str(entb)][np.random.permutation(len(neighbors[str(entb)]))[0]]\n",
    "                        rand_dict[(db.get_node(node).functor,db.get_node(node).args[:-1])]=[randenta,randentb]\n",
    "                    elif entb.find('ent')==-1:\n",
    "                        enta = int(enta[enta.find('ent')+3:])\n",
    "                        randenta = enta\n",
    "                        randentb = neighbors[str(enta)][np.random.permutation(len(neighbors[str(enta)]))[0]]\n",
    "                        rand_dict[(db.get_node(node).functor,db.get_node(node).args[:-1])]=[randenta,randentb]\n",
    "                    else:\n",
    "                        randenta = np.random.permutation(14000)[0]\n",
    "                        randentb = np.random.permutation(14000)[0]\n",
    "                        rand_dict[(db.get_node(node).functor,db.get_node(node).args[:-1])]=[randenta,randentb]\n",
    "\n",
    "        db_term = term2str(db.get_node(node))\n",
    "        if db_term[:db_term.find('(')]=='clause':\n",
    "            data = get_embedding_action(act,predicates,embeddings_enta,embeddings_entb,embeddings_rel,embeddings_rule,randenta,randentb)\n",
    "            actions_to_rerank.append([pos] + data)\n",
    "        pos+=1\n",
    "    Reranked_Meta = pd.DataFrame([])\n",
    "    if len(actions_to_rerank)>1:\n",
    "        actions_to_rerank = pd.DataFrame(actions_to_rerank)\n",
    "        Meta = actions_to_rerank[[0,1,2,3,4]]\n",
    "        Data = actions_to_rerank.drop([0,1,2,3,4],axis=1)\n",
    "        predictions = model.predict_proba('./classification_models/model_transe_50/',Data)\n",
    "        heuristic = predictions[:,-1]\n",
    "        Meta['heuristic'] = heuristic\n",
    "        Meta = Meta.rename(columns={0:'position',1:'rel id',2:'ent a',3:'ent b',4:'rule id'})\n",
    "        uniques = []\n",
    "        for meta_i in range(1,len(Meta)):\n",
    "            row = Meta.iloc[meta_i]\n",
    "            if [row['rel id'],row['ent a'], row['ent b']] not in uniques:\n",
    "                uniques.append([row['rel id'],row['ent a'], row['ent b']])\n",
    "        for unique in uniques:\n",
    "            rel_id = unique[0]\n",
    "            enta = unique[1]\n",
    "            entb = unique[2]\n",
    "            Selected = Meta[(Meta['rel id']==rel_id) & (Meta['ent a']== enta) & (Meta['ent b']==entb)]\n",
    "            Selected = Selected.sort_values(by=['heuristic'])\n",
    "            #print(Selected)\n",
    "            Reranked_Meta = Reranked_Meta.append(Selected)\n",
    "        act_counter = 0\n",
    "        for act_i in range(0,len(actions)):\n",
    "            if act_i not in Reranked_Meta['position']:\n",
    "                reranked_actions.append(actions[act_i])\n",
    "            else:\n",
    "                reranked_actions.append(actions[Reranked_Meta.iloc[act_counter]['position']])\n",
    "                act_counter+=1\n",
    "\n",
    "        return reranked_actions, rand_dict\n",
    "    else:\n",
    "        return actions,rand_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_rerank_results = pd.read_csv('results_no_rerank.csv')\n",
    "#pd_results = pd.read_csv('pd_results.csv')\n",
    "#results = list(pd_results['0'])\n",
    "#no_rerank_results = list(no_rerank_results['no rerank'])\n",
    "results = []\n",
    "for test_sample in range(0,len(test_data)):\n",
    "    try:\n",
    "        print(test_sample)\n",
    "        row = test_data.iloc[test_sample]\n",
    "        rel_id = row['rel id']\n",
    "        ent_a = row['entity a']\n",
    "        ent_b = row['entity b']\n",
    "        query = Term('q'+str(rel_id),Term('ent'+str(ent_a)),Term('ent'+str(ent_b)))\n",
    "        # Perform incremental grounding.\n",
    "        # This is a split-up of the engine.execute method.\n",
    "\n",
    "        # Initialize the engine with options:\n",
    "        #   - unbuffered: don't buffer results internally in the nodes (mimic depth-first construction of target)\n",
    "        #   - rc_first: first process 'result' and 'complete' messages (allows stopping on 'evaluation' message)\n",
    "        #   - label_all: (optional) label all intermediate nodes with their predicate\n",
    "        engine = problog.engine.DefaultEngine(unbuffered=True, rc_first=True, label_all=True)\n",
    "\n",
    "        # Target formula\n",
    "        #   - keep_all: don't collapse non-probabilistic subformula's => only for visualization\n",
    "        target = problog.formula.LogicFormula(keep_all=True)\n",
    "\n",
    "        db = engine.prepare(facts_and_rules_Prolog)\n",
    "\n",
    "        # Start the incremental grounding.\n",
    "        # The result is a list of 'evaluation' actions.\n",
    "        actions = list(reversed(engine.ground_step(db, query, gp=target)))\n",
    "\n",
    "        i = 0\n",
    "        # Execute until no more 'evaluation' actions can be performed.\n",
    "        rand_dict={}\n",
    "        while actions:\n",
    "            actions = engine.execute_step(actions, steps=1, target=target, name=(False, query, 'query'))\n",
    "\n",
    "            # HERE YOU CAN DO WHATEVER YOU WANT WITH THE ACTION LIST\n",
    "\n",
    "            # Below is just generating some output.\n",
    "            i += 1\n",
    "\n",
    "            #print('==== STEP %d ====' % i)\n",
    "\n",
    "\n",
    "            predicates = []\n",
    "            # Go through the engine's stack and extract predicate evaluation nodes ('EvalDefine')\n",
    "            for rec in engine.stack:\n",
    "                if type(rec).__name__ == 'EvalDefine':  # TODO: we should also include 'EvalOr'?\n",
    "                    nodes = set(b for a, b, in rec.results.results)  # 'target' nodes associated with this evaluation node\n",
    "                    predicates.append(problog.logic.Term(rec.call[0], *rec.call[1]))\n",
    "            #print(predicates)\n",
    "            #actions,rand_dict = rerank_actions(actions,predicates,embeddings_enta,embeddings_entb,embeddings_rel,embeddings_rule,model,neighbors,rand_dict)\n",
    "\n",
    "            #for act in actions:\n",
    "            #    print(db.get_node(act[1]))\n",
    "\n",
    "            if type(engine.stack[0]).__name__=='EvalDefine':\n",
    "                trigger_nodes = set(b for a,b, in engine.stack[0].results.results)\n",
    "\n",
    "            #print ('Active predicates:')\n",
    "            active_nodes = set()  # These are the nodes in 'target' that are still active.\n",
    "            # Go through the engine's stack and extract predicate evaluation nodes ('EvalDefine')\n",
    "            for rec in engine.stack:\n",
    "                if type(rec).__name__ == 'EvalDefine':  # TODO: we should also include 'EvalOr'?\n",
    "                    nodes = set(b for a, b, in rec.results.results)  # 'target' nodes associated with this evaluation node\n",
    "                    #print ('\\t', problog.logic.Term(rec.call[0], *rec.call[1]), list(nodes))\n",
    "                    active_nodes |= nodes # union\n",
    "            #print ('Active nodes:', list(active_nodes))\n",
    "\n",
    "            # Visualize and print the current logic program.\n",
    "            #%dotstr target.to_dot(nodeprops={n: 'fillcolor=\"red\"' for n in active_nodes})\n",
    "\n",
    "            if len(list(trigger_nodes))>0:\n",
    "                print(str(test_sample)+ ':solution found ('+str(i)+')')\n",
    "                results.append(i)\n",
    "                pd_results = pd.DataFrame(results)\n",
    "                pd_results.to_csv('pd_results.csv',index=False)\n",
    "                break\n",
    "\n",
    "            if i>1000:\n",
    "                print(str(test_sample)+ ':too long')\n",
    "                results.append(0)\n",
    "                pd_results = pd.DataFrame(results)\n",
    "                pd_results.to_csv('pd_results.csv',index=False)\n",
    "                break\n",
    "\n",
    "            if len(actions)==0:\n",
    "                print(str(test_sample)+ ':solution not found')\n",
    "                results.append(-1)\n",
    "                pd_results = pd.DataFrame(results)\n",
    "                pd_results.to_csv('pd_results.csv',index=False)\n",
    "                break\n",
    "\n",
    "    except:\n",
    "        print('cycle')\n",
    "        results.append(-2)\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rerank_results = pd.read_csv('results_no_rerank.csv')\n",
    "pd_results = pd.read_csv('pd_results.csv')\n",
    "results = list(pd_results['0'])\n",
    "no_rerank_results = list(no_rerank_results['no rerank'])\n",
    "for test_sample in range(1000,len(test_data)):\n",
    "    if no_rerank_results[test_sample]>100:\n",
    "        try:\n",
    "            print(test_sample)\n",
    "            row = test_data.iloc[test_sample]\n",
    "            rel_id = row['rel id']\n",
    "            ent_a = row['entity a']\n",
    "            ent_b = row['entity b']\n",
    "            query = Term('q'+str(rel_id),Term('ent'+str(ent_a)),Term('ent'+str(ent_b)))\n",
    "            # Perform incremental grounding.\n",
    "            # This is a split-up of the engine.execute method.\n",
    "\n",
    "            # Initialize the engine with options:\n",
    "            #   - unbuffered: don't buffer results internally in the nodes (mimic depth-first construction of target)\n",
    "            #   - rc_first: first process 'result' and 'complete' messages (allows stopping on 'evaluation' message)\n",
    "            #   - label_all: (optional) label all intermediate nodes with their predicate\n",
    "            engine = problog.engine.DefaultEngine(unbuffered=True, rc_first=True, label_all=True)\n",
    "\n",
    "            # Target formula\n",
    "            #   - keep_all: don't collapse non-probabilistic subformula's => only for visualization\n",
    "            target = problog.formula.LogicFormula(keep_all=True)\n",
    "\n",
    "            db = engine.prepare(facts_and_rules_Prolog)\n",
    "\n",
    "            # Start the incremental grounding.\n",
    "            # The result is a list of 'evaluation' actions.\n",
    "            actions = list(reversed(engine.ground_step(db, query, gp=target)))\n",
    "\n",
    "            i = 0\n",
    "            # Execute until no more 'evaluation' actions can be performed.\n",
    "            rand_dict={}\n",
    "            while actions:\n",
    "                actions = engine.execute_step(actions, steps=1, target=target, name=(False, query, 'query'))\n",
    "\n",
    "                # HERE YOU CAN DO WHATEVER YOU WANT WITH THE ACTION LIST\n",
    "\n",
    "                # Below is just generating some output.\n",
    "                i += 1\n",
    "\n",
    "                #print('==== STEP %d ====' % i)\n",
    "\n",
    "\n",
    "                predicates = []\n",
    "                # Go through the engine's stack and extract predicate evaluation nodes ('EvalDefine')\n",
    "                for rec in engine.stack:\n",
    "                    if type(rec).__name__ == 'EvalDefine':  # TODO: we should also include 'EvalOr'?\n",
    "                        nodes = set(b for a, b, in rec.results.results)  # 'target' nodes associated with this evaluation node\n",
    "                        predicates.append(problog.logic.Term(rec.call[0], *rec.call[1]))\n",
    "                #print(predicates)\n",
    "                actions,rand_dict = rerank_actions(actions,predicates,embeddings_enta,embeddings_entb,embeddings_rel,embeddings_rule,model,neighbors,rand_dict)\n",
    "\n",
    "                #for act in actions:\n",
    "                #    print(db.get_node(act[1]))\n",
    "\n",
    "                if type(engine.stack[0]).__name__=='EvalDefine':\n",
    "                    trigger_nodes = set(b for a,b, in engine.stack[0].results.results)\n",
    "\n",
    "                #print ('Active predicates:')\n",
    "                active_nodes = set()  # These are the nodes in 'target' that are still active.\n",
    "                # Go through the engine's stack and extract predicate evaluation nodes ('EvalDefine')\n",
    "                for rec in engine.stack:\n",
    "                    if type(rec).__name__ == 'EvalDefine':  # TODO: we should also include 'EvalOr'?\n",
    "                        nodes = set(b for a, b, in rec.results.results)  # 'target' nodes associated with this evaluation node\n",
    "                        #print ('\\t', problog.logic.Term(rec.call[0], *rec.call[1]), list(nodes))\n",
    "                        active_nodes |= nodes # union\n",
    "                #print ('Active nodes:', list(active_nodes))\n",
    "\n",
    "                # Visualize and print the current logic program.\n",
    "                #%dotstr target.to_dot(nodeprops={n: 'fillcolor=\"red\"' for n in active_nodes})\n",
    "\n",
    "                if len(list(trigger_nodes))>0:\n",
    "                    print(str(test_sample)+ ':solution found ('+str(i)+')')\n",
    "                    results.append(i)\n",
    "                    pd_results = pd.DataFrame(results)\n",
    "                    pd_results.to_csv('pd_results.csv',index=False)\n",
    "                    break\n",
    "\n",
    "                if i>1000:\n",
    "                    print(str(test_sample)+ ':too long')\n",
    "                    results.append(0)\n",
    "                    pd_results = pd.DataFrame(results)\n",
    "                    pd_results.to_csv('pd_results.csv',index=False)\n",
    "                    break\n",
    "\n",
    "                if len(actions)==0:\n",
    "                    print(str(test_sample)+ ':solution not found')\n",
    "                    results.append(-1)\n",
    "                    pd_results = pd.DataFrame(results)\n",
    "                    pd_results.to_csv('pd_results.csv',index=False)\n",
    "                    break\n",
    "\n",
    "        except:\n",
    "            print('cycle')\n",
    "            results.append(-2)\n",
    "            pass   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
